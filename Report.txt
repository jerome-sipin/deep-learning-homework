Data Preprocessing:
Targets: Successful
Features: Application type and classification primarily, and the other non EIN/Name columns
Non-features to be removed: EIN and Name columns


Compiling, Training, Evaluating:
For the first run, I used all of the suggested parameters for the model (such as layers, or neurons), as
they were provided in the starter code.

For the second run, I tried to (1) drop less columns from both the application types
and classifications, (2) use less neurons, but more layers, (3) use a different optimizer during
compilation, and (4) use less epochs. I did these because I felt that if there was more data to use,
but less trials, then there would be more chances of getting the desired result. However, no matter
what I tried, the result only changed slightly.


Summary:
The deep learning model gives a 72.6297% accuracy for the predictions. This was gained from
attempting a number of optimizations, including dropping less columns, allowing for more data,
using more layer, using a different optimizer, and using less epochs.

An autoencoder-type model might be suited to handle this type of problem. It is designed for
use in large datasets and is useful for making recommendations, which is what Alphabet Soup Charity is
looking for in this problem. Looking to the TensorFlow documentation for an autoencoder model,
this may be simple enough to implement by simply changing the code for the model creation and compilation.
https://towardsdatascience.com/6-deep-learning-models-10d20afec175
